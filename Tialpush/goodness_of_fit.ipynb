{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Assignment 2:\n",
    "    test the difference between a Binomial/Poisson distribution and a Gaussian distribution as you change the parameters of the Binomial/Poisson. Use: \n",
    "    \n",
    "    1) KS\n",
    "    \n",
    "    2) KL or Chisq\n",
    "    \n",
    "    3) AD\n",
    "    \n",
    "    6 test total (8 if you are ambitious and want to try Pearson's chisq)\n",
    "    \n",
    "    i want you to use the scipy.stats package for this. it will provide tests. For KS and AD you will simply need to generate the distribution the normal will be tested agains: the KS and AD functions in scipy have functionality to test against standard distributions (normal, chisq etc) without you having to code up anything about the distribution itself. KL and chisq do not. I am showing you how to do the AD, KS and KL for binomial, you can replicate it for the poisson or do the Chisq instead of KL.\n",
    "    \n",
    "    even if you just replicate it... PLEASE UNDESRTAND IT! or you will have learned nothing and wasted both of our times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my usual imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\wentao\\\\Downloads'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "\n",
    "import os\n",
    "\n",
    "#this makes my plots pretty! but it is totally not mandatory to do it\n",
    "import json\n",
    "#s = json.load(\"C:/Users/wentao/Downloads/fbb_matplotlibrc.json\" )\n",
    "#pl.rcParams.update(s)\n",
    "\n",
    "#plus importing scipy.stats\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here are the manual pages fo the tests\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html#scipy.stats.kstest\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html#scipy.stats.anderson\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html#scipy.stats.entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i need to figure out what each test function returns. let me run them once to see (and read the manual atthe same time!)\n",
    "\n",
    "i am testing for normal against normal and for a binomial w small n*p against normal. that will give me very different outcomes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal on normal (0.030099001881675247, 0.32020814921677232)\n",
      "normal on normal (0.97694961310401141, array([ 0.574,  0.653,  0.784,  0.914,  1.088]), array([ 15. ,  10. ,   5. ,   2.5,   1. ]))\n",
      "\n",
      "poisson on normal (0.5, 0.0)\n",
      "poisson on normal (179.61433865080653, array([ 0.574,  0.653,  0.784,  0.914,  1.088]), array([ 15. ,  10. ,   5. ,   2.5,   1. ]))\n",
      "[ 1.088]\n",
      "0.976949613104\n"
     ]
    }
   ],
   "source": [
    "#generate the distribution\n",
    "dist_n = np.random.randn(1000)\n",
    "\n",
    "#test ad and ks. those are easy\n",
    "print \"normal on normal\", scipy.stats.kstest(dist_n,'norm')\n",
    "print \"normal on normal\", scipy.stats.anderson(dist_n, dist='norm')\n",
    "print \"\" \n",
    "\n",
    "dist_b = np.random.binomial(1, 0.5, 1000)\n",
    "\n",
    "print \"poisson on normal\", scipy.stats.kstest(dist_b,'norm')\n",
    "print \"poisson on normal\", scipy.stats.anderson(dist_b, dist='norm')\n",
    "\n",
    "threshold = scipy.stats.anderson(dist_n, dist='norm')[1][scipy.stats.anderson(dist_n, dist='norm')[2]==[1.0]]\n",
    "print threshold\n",
    "\n",
    "print scipy.stats.anderson(dist_n, dist='norm')[0]\n",
    "\n",
    "##what is this threshold I am saving??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i will leave it to you to study the outputs in detail, but lets use the statistics. \n",
    "\n",
    "the statistics value will be **larger** if the distributions are **not** likely to be related. \n",
    "think about this in terms of rejection of the NULL hypothesis. \n",
    "\n",
    "#try writing down the null for each test and see if scipy manual agrees with you         (the manual spells out the Null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the KL divergence is a little trickier: i have to pass it the values of the distribution at some x's: that means i have to pass it a **NORMALIZED HOSTOGRAM OF MY DISTRIBUTION**, i.e. derive a PDF from the data. we have done it before!\n",
    "\n",
    "i have to pass it a function for my comparison distribution evaluated at the x-values where my empirical distribution is evaluated.  not just the name of a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distpdf_n, mybins_n, = np.histogram(dist_n, density=True)\n",
    "distpdf_b, mybins_b, = np.histogram(dist_b, density=True)\n",
    "#notice the extra comma on the left side of the '=' sign: that tells numpy take the first two values returned, and throw away the rest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then i have to pass it the functional form of a normal PDF, which is returned by the function scipy.stats.norm for example (a google search would have told you!) \n",
    "evaluated at the bin centers of the histogram from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal on normal 0.00669759395009\n",
      "poisson on normal 1.68574763974\n"
     ]
    }
   ],
   "source": [
    ",#get the bin centers\n",
    "bincenters_n = mybins_n[:-1] + 0.5*(mybins_n[1] - mybins_n[0])\n",
    "bincenters_b = mybins_b[:-1] + 0.5*(mybins_b[1] - mybins_b[0])\n",
    "print \"poisson on normal\", scipy.stats.entropy(distpdf_b, scipy.stats.norm.pdf(bincenters_b)) \n",
    "stpdf_n, scipy.stats.norm.pdf(bincenters_n))  \n",
    "#you can interpret this as a distance: it increases as the distributions diverge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so it looks like a high value is rejecting the NULL that ... (say it in your own words!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BINOMIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#to store the data i am generating an empty array of the size of the values of n i want to test. i do that with np.zeros which takes the length of the array as argument, and the data type as optional argument, but default is float, which is fine by me, so i do not need any argument other then the lenght \n",
    "narray = range(1,50,1)\n",
    "ks_b = np.zeros(len(narray))\n",
    "ad_b = np.zeros(len(narray))\n",
    "kl_b = np.zeros(len(narray))\n",
    "chi2_b = np.zeros(len(narray))\n",
    "\n",
    "\n",
    "def mynorm (x, mu, var):\n",
    "    return scipy.stats.norm.cdf(x, loc=mu, scale=var)\n",
    "\n",
    "#then i put the tests in a for loop so that i can generate a distribution for given parameters once, \n",
    "#and run all tests against it\n",
    "#now the valus that i want to plot depends on how i intend to describe the plot, and viceversa. \n",
    "\n",
    "#here is one way to plot it, knowing the values i get for the tests when \n",
    "#i throw in a poisson distribution with low l and compare it with a gaussian and \n",
    "#assuming that that is a vary bad match\n",
    "p=0.5\n",
    "for i,n in enumerate(narray):\n",
    "    p=0.1 #parameter for the binomial, my arbitrary choice\n",
    "    #generate the distribution\n",
    "    dist = np.random.binomial(n, p, 1000)\n",
    "    #run the tests. \n",
    " \n",
    "    ks_b[i] = scipy.stats.kstest(dist, mynorm, args=(n*p, n*p*(1.0-p)))[0]\n",
    "    ad_b[i] = scipy.stats.anderson(dist, dist='norm')[0]\n",
    "    \n",
    "    \n",
    "    # for KL and Pearson's chisq I have to simulate the normal distribution as well\n",
    "    mybins=np.linspace(min(dist),max(dist), 10) \n",
    "    bincenters = mybins[:-1]+0.5*(mybins[1]-mybins[0])\n",
    "\n",
    "    #when i was coding this up something was wrong. i put some plots in to figure out what... just so you know.\n",
    "    #if i%10 == 0: \n",
    "    #    print n\n",
    "    #    pl.figure()\n",
    "    #    pl.hist(dist, bins=mybins)\n",
    "    #    pl.plot(bincenters, 1000*scipy.stats.norm.pdf(bincenters, loc=n*p, scale=n*p*(1-p)))\n",
    "    kl_b [i] =  scipy.stats.entropy(np.histogram(dist, bins=mybins)[0], scipy.stats.norm.pdf(bincenters, loc=n*p, scale=n*p*(1.0-p)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#this is just one way to plot it. get creative and find the best way to show your result!\n",
    "then give me a cell in which you write a caption for the figures you plot. think about how the captions are written in the paper  you have to read for this week assignment: a caption must describe the plot sufficiently well that i do not need to read the paper to understand it. though it will generally lack details of the how and why you see what you see. those are in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = pl.figure(figsize = (15,5))\n",
    "fig.add_subplot(131)\n",
    "pl.plot(narray, ks_b, label='KS')\n",
    "pl.legend()\n",
    "\n",
    "fig.add_subplot(132)\n",
    "pl.plot(narray, ad_b,  label='AD')\n",
    "pl.plot([narray[0], narray[-1]],[threshold, threshold])\n",
    "pl.plot()\n",
    "pl.plot()\n",
    "pl.legend()\n",
    "\n",
    "fig.add_subplot(133)\n",
    "pl.plot(narray, kl_b, label='K-L ')\n",
    "\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#POISSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#to store the data i am generating an empty array of the size of the values of n i want to test. i do that with np.zeros which takes the length of the array as argument, and the data type as optional argument, but default is float, which is fine by me, so i do not need any argument other then the lenght \n",
    "narray = range(1,50,1)\n",
    "ks_b = np.zeros(len(narray))\n",
    "ad_b = np.zeros(len(narray))\n",
    "kl_b = np.zeros(len(narray))\n",
    "chi2_b = np.zeros(len(narray))\n",
    "\n",
    "\n",
    "#then i put the tests in a for loop so that i can generate a distribution for given parameters once, \n",
    "#and run all tests against it\n",
    "#now the valus that i want to plot depends on how i intend to describe the plot, and viceversa. \n",
    "\n",
    "#here is one way to plot it, knowing the values i get for the tests when \n",
    "#i throw in a poisson distribution with low l and compare it with a gaussian and \n",
    "#assuming that that is a vary bad match\n",
    "\n",
    "for i,n in enumerate(narray):\n",
    "    p=0.1 #parameter for the binomial, my arbitrary choice\n",
    "    #generate the distribution\n",
    "    dist = np.random.poisson(n, 1000)\n",
    "    #run the tests. \n",
    "    ks_b[i] = scipy.stats.kstest(dist, mynorm, args=(n, n))[0]\n",
    "    ad_b[i] = scipy.stats.anderson(dist, dist='norm')[0]\n",
    "    \n",
    "        \n",
    "    # for KL and Pearson's chisq I have to simulate the normal distribution as well\n",
    "    mybins = np.linspace(min(dist),max(dist), 10) \n",
    "    bincenters = mybins[:-1]+0.5*(mybins[1]-mybins[0])\n",
    " \n",
    "    kl_b [i] =  scipy.stats.entropy(np.histogram(dist, bins=mybins)[0], scipy.stats.norm.pdf(bincenters, loc=n*p, scale=n*p*(1.0-p)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = pl.figure(figsize = (15,5))\n",
    "fig.add_subplot(131)\n",
    "pl.plot(narray, ks_b, label='KS')\n",
    "pl.legend()\n",
    "\n",
    "fig.add_subplot(132)\n",
    "pl.plot(narray, ad_b,  label='AD')\n",
    "pl.plot([narray[0], narray[-1]],[threshold, threshold])\n",
    "pl.plot()\n",
    "pl.plot()\n",
    "pl.legend()\n",
    "\n",
    "fig.add_subplot(133)\n",
    "pl.plot(narray, kl_b, label='K-L ')\n",
    "\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##TOTALLY OPTIONAL fun with KL: the KL divergence is a measure of how much approximation you have to do to represent one distribution with the other. you can see what it looks point by point, before you integrate and get a single distance number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(15,15))\n",
    "\n",
    "p = lambda x, mu : scipy.stats.distributions.poisson.pmf(x,mu)\n",
    "q = lambda x, mu : scipy.stats.distributions.norm.pdf(x-mu)\n",
    "c = lambda x, mu : scipy.stats.distributions.chi2.pdf(x, df=mu)\n",
    "f = lambda x, mu : scipy.stats.distributions.f.pdf(x,100,100)\n",
    "\n",
    "Dkl = lambda x ,mu, p :  p(x, mu)* np.log10(q(x, mu)) + p(x, mu) *np.log10(p(x, mu))\n",
    "\n",
    "def model(x) : \n",
    "    return 1./(s*np.sqrt(2*np.pi))*exp(-((x-m)/2./2./s)**2)\n",
    "\n",
    "x=np.linspace(-10,10,100)\n",
    "pl.plot(x, q(x,1), label='norm')\n",
    "pl.plot(x, p(x,1), label='poisson')\n",
    "pl.plot(x, Dkl(x, 1, p), '-.', label = 'KL - poisson')\n",
    "\n",
    "#print np.nansum(Dkl(np.linspace(0,30,1000), 1, p))\n",
    "\n",
    "\n",
    "pl.plot(x, c(x,1), label='f')\n",
    "pl.plot(x,Dkl(x, 1, c), '--', label = 'KL - chi2')\n",
    "\n",
    "\n",
    "\n",
    "#print np.nansum(Dkl(np.linspace(0,30,1000), 1, c))\n",
    "\n",
    "\n",
    "pl.plot(x, f(x,1), label='F')\n",
    "pl.plot(x,Dkl(x, 1, f), '--', label = 'KL - F')\n",
    "\n",
    "#print np.nansum(Dkl(np.linspace(0,30,1000), 1, f))\n",
    "bins = np.arange(0, 99, 10)\n",
    "\n",
    "pl.xlim(0,10)\n",
    "pl.legend( fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##WRITE A CAPTION HERE IN THIS CELL.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
